{"cells":[{"cell_type":"markdown","source":["### Spark session configuration\r\n","This cell sets Spark session settings to enable _Verti-Parquet_ and _Optimize on Write_. More details about _Verti-Parquet_ and _Optimize on Write_ in tutorial document."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7438444b-95d1-45de-9aef-6b2bc01aae0d"},{"cell_type":"code","source":["# Copyright (c) Microsoft Corporation.\r\n","# Licensed under the MIT License.\r\n","\r\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"4afff20a-4c28-49eb-9a6a-db666c9dc172","statement_id":3,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-26T07:01:27.0563137Z","session_start_time":"2023-05-26T07:01:27.3110257Z","execution_start_time":"2023-05-26T07:01:37.6418621Z","execution_finish_time":"2023-05-26T07:01:39.0216927Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"c2144e88-9ea7-4607-a089-a950a6695016"},"text/plain":"StatementMeta(, 4afff20a-4c28-49eb-9a6a-db666c9dc172, 3, Finished, Available)"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"b18d4666-5af1-49fb-8b41-c560f9b09e43"},{"cell_type":"markdown","source":["### Fact - Sale\r\n","\r\n","This cell reads raw data from the _Files_ section of the lakehouse, adds additional columns for different date parts and the same information is being used to create partitioned fact delta table."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0f7a3a0f-a44e-4df2-864b-55676ce77163"},{"cell_type":"code","source":["from pyspark.sql.functions import col, year, month, quarter\r\n","\r\n","table_name = 'fact_sale'\r\n","\r\n","df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/fact_sale_1y_full')\r\n","df = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\r\n","df = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\r\n","df = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\r\n","\r\n","df.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"4afff20a-4c28-49eb-9a6a-db666c9dc172","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-26T07:01:41.2848969Z","session_start_time":null,"execution_start_time":"2023-05-26T07:01:41.7130094Z","execution_finish_time":"2023-05-26T07:03:00.3694388Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":10,"UNKNOWN":0,"FAILED":0},"jobs":[{"dataWritten":0,"dataRead":4729,"rowCount":50,"jobId":16,"name":"toString at String.java:2994","description":"Delta: Job group for statement 4:\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'fact_sale'\n\ndf = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/fact_sale_1y_full')\ndf = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name): Compute snapshot for version: 1","submissionTime":"2023-05-26T07:03:00.043GMT","completionTime":"2023-05-26T07:03:00.075GMT","stageIds":[27,25,26],"jobGroup":"4","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4729,"dataRead":12542,"rowCount":66,"jobId":15,"name":"toString at String.java:2994","description":"Delta: Job group for statement 4:\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'fact_sale'\n\ndf = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/fact_sale_1y_full')\ndf = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name): Compute snapshot for version: 1","submissionTime":"2023-05-26T07:02:59.510GMT","completionTime":"2023-05-26T07:03:00.024GMT","stageIds":[24,23],"jobGroup":"4","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":12542,"dataRead":18895,"rowCount":32,"jobId":14,"name":"toString at String.java:2994","description":"Delta: Job group for statement 4:\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'fact_sale'\n\ndf = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/fact_sale_1y_full')\ndf = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name): Compute snapshot for version: 1","submissionTime":"2023-05-26T07:02:59.171GMT","completionTime":"2023-05-26T07:02:59.274GMT","stageIds":[22],"jobGroup":"4","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":5812,"rowCount":6,"jobId":13,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 4:\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'fact_sale'\n\ndf = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/fact_sale_1y_full')\ndf = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name)","submissionTime":"2023-05-26T07:02:58.434GMT","completionTime":"2023-05-26T07:02:58.591GMT","stageIds":[20,21],"jobGroup":"4","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":226326191,"dataRead":562194776,"rowCount":100301686,"jobId":12,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 4:\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'fact_sale'\n\ndf = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/fact_sale_1y_full')\ndf = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name)","submissionTime":"2023-05-26T07:02:01.325GMT","completionTime":"2023-05-26T07:02:58.292GMT","stageIds":[19,18],"jobGroup":"4","status":"SUCCEEDED","numTasks":15,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":11,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":562194776,"dataRead":362243868,"rowCount":100301686,"jobId":11,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 4:\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'fact_sale'\n\ndf = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/fact_sale_1y_full')\ndf = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name)","submissionTime":"2023-05-26T07:01:44.863GMT","completionTime":"2023-05-26T07:02:01.274GMT","stageIds":[17],"jobGroup":"4","status":"SUCCEEDED","numTasks":11,"numActiveTasks":0,"numCompletedTasks":11,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":11,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4724,"rowCount":50,"jobId":10,"name":"toString at String.java:2994","description":"Delta: Job group for statement 4:\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'fact_sale'\n\ndf = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/fact_sale_1y_full')\ndf = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name): Compute snapshot for version: 0","submissionTime":"2023-05-26T07:01:44.593GMT","completionTime":"2023-05-26T07:01:44.636GMT","stageIds":[15,16,14],"jobGroup":"4","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4724,"dataRead":6153,"rowCount":57,"jobId":9,"name":"toString at String.java:2994","description":"Delta: Job group for statement 4:\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'fact_sale'\n\ndf = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/fact_sale_1y_full')\ndf = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name): Compute snapshot for version: 0","submissionTime":"2023-05-26T07:01:43.712GMT","completionTime":"2023-05-26T07:01:44.571GMT","stageIds":[12,13],"jobGroup":"4","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":6153,"dataRead":10028,"rowCount":14,"jobId":8,"name":"toString at String.java:2994","description":"Delta: Job group for statement 4:\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'fact_sale'\n\ndf = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/fact_sale_1y_full')\ndf = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name): Compute snapshot for version: 0","submissionTime":"2023-05-26T07:01:43.412GMT","completionTime":"2023-05-26T07:01:43.511GMT","stageIds":[11],"jobGroup":"4","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":7,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 4:\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'fact_sale'\n\ndf = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/fact_sale_1y_full')\ndf = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\ndf = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name)","submissionTime":"2023-05-26T07:01:42.182GMT","completionTime":"2023-05-26T07:01:42.898GMT","stageIds":[10],"jobGroup":"4","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"4e0766e0-1435-4254-a57b-767e12ae0fd3"},"text/plain":"StatementMeta(, 4afff20a-4c28-49eb-9a6a-db666c9dc172, 4, Finished, Available)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"2feb998f-65d1-472e-822f-60347b94f615"},{"cell_type":"markdown","source":["### Dimensions\r\n","This cell creates a function to read raw data from the _Files_ section of the lakehouse for the table name passed as a parameter. Next, it creates a list of dimension tables. Finally, it has a _for loop_ to loop through the list of tables and call above function with each table name as parameter to read data for that specific table and create delta table."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"70429ced-2c38-4baa-9bc1-bb2a21550a5e"},{"cell_type":"code","source":["from pyspark.sql.types import *\r\n","\r\n","def loadFullDataFromSource(table_name):\r\n","    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\r\n","    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\r\n","\r\n","full_tables = [\r\n","    'dimension_city',\r\n","    'dimension_date',\r\n","    'dimension_employee',\r\n","    'dimension_stock_item'\r\n","    ]\r\n","\r\n","for table in full_tables:\r\n","    loadFullDataFromSource(table)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"4afff20a-4c28-49eb-9a6a-db666c9dc172","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2023-05-26T07:04:38.3325122Z","session_start_time":null,"execution_start_time":"2023-05-26T07:04:38.815821Z","execution_finish_time":"2023-05-26T07:04:54.1554024Z","spark_jobs":{"numbers":{"RUNNING":1,"SUCCEEDED":38,"UNKNOWN":0,"FAILED":0},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":55,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 1","submissionTime":"2023-05-26T07:04:52.666GMT","stageIds":[96,95],"jobGroup":"5","status":"RUNNING","numTasks":52,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":1,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4545,"dataRead":6181,"rowCount":14,"jobId":54,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 1","submissionTime":"2023-05-26T07:04:52.472GMT","completionTime":"2023-05-26T07:04:52.554GMT","stageIds":[94],"jobGroup":"5","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":2581,"rowCount":3,"jobId":53,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-05-26T07:04:51.780GMT","completionTime":"2023-05-26T07:04:51.902GMT","stageIds":[93,92],"jobGroup":"5","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":21581,"dataRead":36536,"rowCount":1344,"jobId":52,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-05-26T07:04:51.327GMT","completionTime":"2023-05-26T07:04:51.635GMT","stageIds":[90,91],"jobGroup":"5","status":"SUCCEEDED","numTasks":5,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":4,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":36536,"dataRead":56776,"rowCount":1344,"jobId":51,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-05-26T07:04:51.160GMT","completionTime":"2023-05-26T07:04:51.284GMT","stageIds":[89],"jobGroup":"5","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4633,"rowCount":50,"jobId":50,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-05-26T07:04:50.978GMT","completionTime":"2023-05-26T07:04:51.007GMT","stageIds":[88,86,87],"jobGroup":"5","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4633,"dataRead":2376,"rowCount":54,"jobId":49,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-05-26T07:04:50.614GMT","completionTime":"2023-05-26T07:04:50.958GMT","stageIds":[84,85],"jobGroup":"5","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2376,"dataRead":3936,"rowCount":8,"jobId":48,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-05-26T07:04:50.446GMT","completionTime":"2023-05-26T07:04:50.500GMT","stageIds":[83],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":47,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-05-26T07:04:50.116GMT","completionTime":"2023-05-26T07:04:50.197GMT","stageIds":[82],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4459,"rowCount":50,"jobId":46,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 1","submissionTime":"2023-05-26T07:04:49.886GMT","completionTime":"2023-05-26T07:04:49.910GMT","stageIds":[81,79,80],"jobGroup":"5","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4459,"dataRead":3584,"rowCount":57,"jobId":45,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 1","submissionTime":"2023-05-26T07:04:49.491GMT","completionTime":"2023-05-26T07:04:49.871GMT","stageIds":[78,77],"jobGroup":"5","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":3584,"dataRead":3695,"rowCount":14,"jobId":44,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 1","submissionTime":"2023-05-26T07:04:49.255GMT","completionTime":"2023-05-26T07:04:49.311GMT","stageIds":[76],"jobGroup":"5","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":2018,"rowCount":3,"jobId":43,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-05-26T07:04:48.674GMT","completionTime":"2023-05-26T07:04:48.769GMT","stageIds":[74,75],"jobGroup":"5","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":7348,"dataRead":7711,"rowCount":426,"jobId":42,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-05-26T07:04:48.288GMT","completionTime":"2023-05-26T07:04:48.572GMT","stageIds":[72,73],"jobGroup":"5","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":5,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":7711,"dataRead":22728,"rowCount":426,"jobId":41,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-05-26T07:04:48.111GMT","completionTime":"2023-05-26T07:04:48.250GMT","stageIds":[71],"jobGroup":"5","status":"SUCCEEDED","numTasks":5,"numActiveTasks":0,"numCompletedTasks":5,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":5,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4454,"rowCount":50,"jobId":40,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-05-26T07:04:47.961GMT","completionTime":"2023-05-26T07:04:47.991GMT","stageIds":[70,68,69],"jobGroup":"5","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4454,"dataRead":1805,"rowCount":54,"jobId":39,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-05-26T07:04:47.521GMT","completionTime":"2023-05-26T07:04:47.944GMT","stageIds":[66,67],"jobGroup":"5","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1805,"dataRead":2231,"rowCount":8,"jobId":38,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-05-26T07:04:47.361GMT","completionTime":"2023-05-26T07:04:47.415GMT","stageIds":[65],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":37,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-05-26T07:04:47.030GMT","completionTime":"2023-05-26T07:04:47.100GMT","stageIds":[64],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4452,"rowCount":50,"jobId":36,"name":"toString at String.java:2994","description":"Delta: Job group for statement 5:\nfrom pyspark.sql.types import *\n\ndef loadFullDataFromSource(table_name):\n    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n\nfull_tables = [\n    'dimension_city',\n    'dimension_date',\n    'dimension_employee',\n    'dimension_stock_item'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 1","submissionTime":"2023-05-26T07:04:46.799GMT","completionTime":"2023-05-26T07:04:46.826GMT","stageIds":[63,61,62],"jobGroup":"5","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"f45fa1b3-a7f0-4f11-966f-43ac23cdccd9"},"text/plain":"StatementMeta(, 4afff20a-4c28-49eb-9a6a-db666c9dc172, 5, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"ec499237-ecee-417d-a109-de3c51951314"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"default_lakehouse":"832d2d26-0b61-472a-9028-9d2860e15071","default_lakehouse_name":"FabricLakehouse","default_lakehouse_workspace_id":"67b89416-5490-4a2c-9133-3d0cf579aa1f","known_lakehouses":[{"id":"832d2d26-0b61-472a-9028-9d2860e15071"}]}}},"nbformat":4,"nbformat_minor":5}